{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/deth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from east.asts import base\n",
    "from east import utils\n",
    "import glob\n",
    "import errno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing.dummy import Pool\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_sentence(sentence):\n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    stripped = sentence.translate(table)\n",
    "    tokens = word_tokenize(stripped)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(word) for word in words]\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_substrings(text, k):\n",
    "    substrings_array = []\n",
    "    for i in range(len(text) - k + 1):\n",
    "        substring = \" \".join(text[i : i + k])\n",
    "        substrings_array.append(substring)\n",
    "        i+=1\n",
    "    return substrings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(text, leafs):\n",
    "    new = []\n",
    "    if (len(text)) != 0:\n",
    "        ast = base.AST.get_ast(text)\n",
    "        # Compute the relevance of a keyphrase to the text collection indexed by this AST.\n",
    "        # The relevance score will always be in [0; 1]\n",
    "        for j in range(len(leafs)):\n",
    "            try:\n",
    "                score = ast.score(leafs[j])\n",
    "                new.append(score)\n",
    "            except ZeroDivisionError:\n",
    "                print(\"problem\\n\",text, '\\n', leafs[j])\n",
    "    else:\n",
    "        new = np.zeros(len(leafs))\n",
    "    return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(glob.glob('./Data/*.csv'))\n",
    "    \n",
    "li = []\n",
    "\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lists of requirements,responsibiliries, conditions from files\n",
    "f = open('./resp_file.txt', 'r')\n",
    "responsibilities = f.read().split('\\n')\n",
    "f = open('./req_file.txt', 'r')\n",
    "requirements = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open('./workenv_file.txt', 'r')\n",
    "conditions = f.read().split('\\n')\n",
    "f.close()\n",
    "requirements = requirements[:-1]\n",
    "conditions = conditions[:-1]\n",
    "responsibilities = responsibilities[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progressbar import Percentage, ProgressBar,Bar,ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100% Time: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "someMaxValue=len(requirements)\n",
    "pbar = ProgressBar(widgets=[Bar('>', '[', ']'), ' ',\n",
    "                                            Percentage(), ' ',\n",
    "                                            ETA()],maxval=someMaxValue)\n",
    "clear_req = []\n",
    "for sent in pbar(requirements):\n",
    "    cleared = ' '.join(clear_sentence(sent))\n",
    "    if cleared != '':\n",
    "        clear_req.append(cleared)\n",
    "    else:\n",
    "        clear_req.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100% Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "someMaxValue=len(responsibilities)\n",
    "pbar = ProgressBar(widgets=[Bar('>', '[', ']'), ' ',\n",
    "                                            Percentage(), ' ',\n",
    "                                            ETA()],maxval=someMaxValue)\n",
    "clear_resp = []\n",
    "for sent in pbar(responsibilities):\n",
    "    cleared = ' '.join(clear_sentence(sent))\n",
    "    if cleared != '':\n",
    "        clear_resp.append(cleared)\n",
    "    else:\n",
    "        clear_resp.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100% Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "someMaxValue=len(conditions)\n",
    "pbar = ProgressBar(widgets=[Bar('>', '[', ']'), ' ',\n",
    "                                            Percentage(), ' ',\n",
    "                                            ETA()],maxval=someMaxValue)\n",
    "clear_cond = []\n",
    "for sent in pbar(conditions):\n",
    "    cleared = ' '.join(clear_sentence(sent))\n",
    "    if cleared != '':\n",
    "        clear_cond.append(cleared)\n",
    "    else:\n",
    "        clear_cond.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100% Time: 0:01:31\n"
     ]
    }
   ],
   "source": [
    "texts = frame['Job Description']\n",
    "\n",
    "someMaxValue=len(texts)\n",
    "pbar = ProgressBar(widgets=[Bar('>', '[', ']'), ' ',\n",
    "                                            Percentage(), ' ',\n",
    "                                            ETA()],maxval=someMaxValue)\n",
    "\n",
    "strings_collection = []\n",
    "\n",
    "for text in pbar(texts):\n",
    "    try:\n",
    "        strings_collection.append(text_substrings(clear_sentence(text), 7))\n",
    "    except(AttributeError):\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100% Time: 5:24:59\n"
     ]
    }
   ],
   "source": [
    "req_count = []\n",
    "someMaxValue=len(strings_collection)\n",
    "pbar = ProgressBar(widgets=[Bar('>', '[', ']'), ' ',\n",
    "                                            Percentage(), ' ',\n",
    "                                            ETA()],maxval=someMaxValue)\n",
    "for strings in pbar(strings_collection):\n",
    "    req_count.append(calculate(strings, clear_req))\n",
    "\n",
    "df_req = pd.DataFrame(req_count, index=np.arange(len(texts)), columns=requirements)\n",
    "df_req.to_csv(\"./Data/Result_req.csv\")\n",
    "del df_req\n",
    "del req_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100% Time: 8:03:58\n"
     ]
    }
   ],
   "source": [
    "resp_count = []\n",
    "someMaxValue=len(strings_collection)\n",
    "pbar = ProgressBar(widgets=[Bar('>', '[', ']'), ' ',\n",
    "                                            Percentage(), ' ',\n",
    "                                            ETA()],maxval=someMaxValue)\n",
    "for strings in pbar(strings_collection):\n",
    "    resp_count.append(calculate(strings, clear_resp))\n",
    "\n",
    "df_resp = pd.DataFrame(resp_count, index=np.arange(len(texts)), columns=responsibilities)\n",
    "df_resp.to_csv(\"./Result_resp.csv\")\n",
    "del df_resp\n",
    "del resp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_count = []\n",
    "for strings in pbar(strings_collection):\n",
    "#     req_count.append(calculate(strings, clear_req))\n",
    "#     resp_count.append(calculate(strings, clear_resp))\n",
    "    cond_count.append(calculate(strings, clear_cond))\n",
    "df_count = pd.DataFrame(cond_count, index=np.arange(len(texts)), columns=conditions)\n",
    "df_count.to_csv(\"./Data/Result_we.csv\")\n",
    "del df_count\n",
    "del cound_count\n",
    "# df_req.to_csv(\"./Data/Result_req.csv\")\n",
    "# df_resp.to_csv(\"./Data/Result_resp.csv\")\n",
    "# df_count.to_csv(\"./Data/Result_count.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
